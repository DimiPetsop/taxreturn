---
title: "taxreturn"
author: "Alexander Piper"
date: "2018/12/11"
output:
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}

# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/taxreturn')
opts_chunk$set(dev = 'png')

```

### Fucntions to add

#Database creation
get_database(options="bold, "genbank" etc) + some hmmer cleaning?
clean_database(ie automated filters)
clean_alignment(remove badly aligend sequences)
format_datbase(options="blast","RDP","q2","IDTAXA" etc))
add_sequences(add local db)
Trim_to_primer_regions - Using insect r package HMM's

#Sample Sheet & Index switching
Samplesheet_new(options=Miseq, hiseq etc)
Samplesheet_allcombinations(expand out all combinations)
switchrate_allcomb(calculate switchrate from expected vs observed)
switchrate_control(calculate switchrate from control - allow selecting control)

#primer evaluation
evaluate_primerbind(use hmm similar to insect r package to locate bind site, then primer miner evaluation)
plot_primer - Plot the above output

#analysis
Mostly handled by DADA2
evaluate_bias and calibrate_bias - Imported from 

#plotting
Mostly hanadled by phyloseq
summarise_Taxa(samples=c()) - similar to current summarise taxa but allow subsettign to desired samples
Plot_expectedobserved(allow comparison of fit)

#summarising
output detection tables etc - reports


How many sequences were removed at each stage - Plot of this (Histogram of reductions)
How many sequences were dereplicated - Plot of dataset redundancy (Histogram of species had 1,2,3,4,5,6,7,8,9,10+ etc)


Use aphid R package which incorporates HMM's to extract COI from download clean up sequence alignments!
https://rdrr.io/cran/aphid/man/aphid.html

Could then follow it with cleanup scripts from the muray darling paper?

## Setup

```{r message=FALSE}
##Load Necessary packages
sapply(c("rentrez","bold", "seqinr","taxonomizr", "tidyverse", "data.table","DECIPHER","insect","aphid","taxize","r.utils"), require, character.only = TRUE)
```






```{r fix old file}
merged <-  readFASTA("merged_cleaned_inhouse.fa")

#Remove specific mislabelled sequences

names <- as_tibble(unlist(names(merged)))
names <- names %>%
  dplyr::filter(!str_detect(value, fixed("_x_"))) %>%
  dplyr::filter(!str_detect(value, fixed("KR654855.1"))) %>%
  dplyr::filter(!str_detect(value, fixed("EU493618.1"))) %>%
  dplyr::filter(!str_detect(value, fixed("AUSBC806-12"))) 


rm_keywords <- names$value
merged2 <- merged[names(merged) %in% rm_keywords]


writeFASTA(merged2,"merged_trimmed.fa")


merged <-  readFASTA("merged_trimmed.fa")

#Get taxonomy database
db <- taxonomy(db = "NCBI", synonyms = FALSE)

#Cluster OTU's and flag mislabelled taxa
cleaned <- purge(merged, db, level = "Genus", confidence = 0.8, cores = 12,
  quiet = FALSE, threshold = 0.97, method = "farthest")
writeFASTA(cleaned,"merged_cleaned.fa")

```






# Prune sequences & Train IDTaxa classfier

Prune seqs, merge in our samples, train classifier

```{r}
# specify the path to your file of training sequences:
seqs_path <- "merged_cleaned_inhouse.fa"
# read the sequences into memory
seqs <- readDNAStringSet(seqs_path)

# As taxonomies are encoded in the sequence names rather than a separate file, use:
taxid <- NULL
seqs <- RemoveGaps(seqs)
seqs <- OrientNucleotides(seqs)

# obtain the taxonomic assignments
names <- names(seqs) %>%
  str_split_fixed(";",n=2)
names <- names[,2]
names <- str_replace_all(names,"Eukaryota;","Root;")

names(seqs) <- names
groups <- names # sequence names

# assume the taxonomy begins with 'Root;' 


groups <- gsub("(.*)(Root;)", "\\2", groups) # extract the group label - May need to use a wild card for the root label as its split by 3 genes!
groupCounts <- table(groups)
u_groups <- names(groupCounts) # unique groups
length(u_groups) # number of groups
 
# Pruning training set

maxGroupSize <- 10 # max sequences per label (>= 1)
remove <- logical(length(seqs))
for (i in which(groupCounts > maxGroupSize)) {
  index <- which(groups==u_groups[i])
  keep <- sample(length(index),
  maxGroupSize)
  remove[index[-keep]] <- TRUE
}
sum(remove) # number of sequences eliminated


# Training the classifier
maxIterations <- 3 # must be >= 1
allowGroupRemoval <- TRUE
probSeqsPrev <- integer() # suspected problem sequences from prior iteration
for (i in seq_len(maxIterations)) {
  cat("Training iteration: ", i, "\n", sep="")
  # train the classifier
  trainingSet <- LearnTaxa(seqs[!remove],
  names(seqs)[!remove],
  taxid)
  
  # look for problem sequences
  probSeqs <- trainingSet$problemSequences$Index
  if (length(probSeqs)==0) {
    cat("No problem sequences remaining.\n")
    break
  } else if (length(probSeqs)==length(probSeqsPrev) &&
  all(probSeqsPrev==probSeqs)) {
    cat("Iterations converged.\n")
    break
    }
  if (i==maxIterations)
  break
  probSeqsPrev <- probSeqs
  
  # remove any problem sequences
  index <- which(!remove)[probSeqs]
  remove[index] <- TRUE # remove all problem sequences
  if (!allowGroupRemoval) {
    # replace any removed groups
    missing <- !(u_groups %in% groups[!remove])
    missing <- u_groups[missing]
    if (length(missing) > 0) {
    index <- index[groups[index] %in% missing]
    remove[index] <- FALSE # don't remove
    }
  }
}
sum(remove) # total number of sequences eliminated
length(probSeqs) # number of remaining problem sequences

# View the results of training

trainingSet
plot(trainingSet)

#Write out training set

saveRDS(trainingSet,file="reference/merged_arthropoda_idtaxa.rds")
```
